{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "1. Dimensionality reduction of data through PCA\n",
    "2. (AR)HMM to quantify dynamics of (dim-reduced) activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('white')\n",
    "sns.set_context('talk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "expt_id = '190424_f3'  # '190424_f3' (run+feed) | '180824_f3r1'\n",
    "# expt_id = '180824_f3r1'\n",
    "if expt_id == '190424_f3':\n",
    "    file_name = 'runAndFeedSample.mat'\n",
    "elif expt_id == '180824_f3r1':\n",
    "    file_name = 'runningSample.mat'\n",
    "file_path = os.path.join('/home/mattw/Dropbox/research-code/', file_name)\n",
    "mat_contents = sio.loadmat(file_path)\n",
    "# trialFlag: indexes running/feeding/running components of the experiment\n",
    "# dOO: ratiometric dF/F for every active cell\n",
    "# A: spatial footprint of these cells\n",
    "# legs, stim, and feed: behavioral data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocess data - split into training/testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the segments of training data\n",
    "def split_trials(\n",
    "        n_trials, rng_seed=0, trials_tr=5, trials_val=1, trials_test=1, trials_gap=1):\n",
    "    \"\"\"\n",
    "    Split trials into train/val/test blocks.\n",
    "\n",
    "    The data is split into blocks that have gap trials between tr/val/test:\n",
    "    train tr | gap tr | val tr | gap tr | test tr | gap tr\n",
    "\n",
    "    Args:\n",
    "        n_trials (int): number of trials to use in the split\n",
    "        rng_seed (int): numpy random seed for reproducibility\n",
    "        trials_tr (int): number of train trials per block\n",
    "        trials_val (int): number of validation trials per block\n",
    "        trials_test (int): number of test trials per block\n",
    "        trials_gap (int): number of gap trials between tr/val/test; there will be\n",
    "            a total of 3 * `trials_gap` gap trials per block\n",
    "\n",
    "    Returns:\n",
    "        (dict)\n",
    "    \"\"\"\n",
    "\n",
    "    # same random seed for reproducibility\n",
    "    np.random.seed(rng_seed)\n",
    "\n",
    "    tr_per_block = \\\n",
    "        trials_tr + trials_gap + trials_val + trials_gap + trials_test + trials_gap\n",
    "\n",
    "    n_blocks = int(np.floor(n_trials / tr_per_block))\n",
    "    leftover_trials = n_trials - tr_per_block * n_blocks\n",
    "    if leftover_trials > 0:\n",
    "        offset = np.random.randint(0, high=leftover_trials)\n",
    "    else:\n",
    "        offset = 0\n",
    "    indxs_block = np.random.permutation(n_blocks)\n",
    "\n",
    "    batch_indxs = {'train': [], 'test': [], 'val': []}\n",
    "    for block in indxs_block:\n",
    "\n",
    "        curr_tr = block * tr_per_block + offset\n",
    "        batch_indxs['train'].append(np.arange(curr_tr, curr_tr + trials_tr))\n",
    "        curr_tr += (trials_tr + trials_gap)\n",
    "        batch_indxs['val'].append(np.arange(curr_tr, curr_tr + trials_val))\n",
    "        curr_tr += (trials_val + trials_gap)\n",
    "        batch_indxs['test'].append(np.arange(curr_tr, curr_tr + trials_test))\n",
    "\n",
    "    for dtype in ['train', 'val', 'test']:\n",
    "        batch_indxs[dtype] = np.concatenate(batch_indxs[dtype], axis=0)\n",
    "\n",
    "    return batch_indxs\n",
    "\n",
    "def zscore(data):\n",
    "    \"\"\"zscore over axis 0\"\"\"\n",
    "    std = np.std(data, axis=0)\n",
    "    mean = np.mean(data, axis=0)\n",
    "    return (np.copy(data) - mean) / std\n",
    "\n",
    "def cluster(data):\n",
    "    \"\"\"reorder axis 1 of a T x N matrix\"\"\"\n",
    "    from sklearn.decomposition import PCA\n",
    "    from scipy.cluster import hierarchy\n",
    "\n",
    "    # fit pca to get a feature vector for each neuron\n",
    "    pca = PCA(n_components=40)\n",
    "    pca.fit(data_run)\n",
    "    features = pca.components_.T\n",
    "\n",
    "    # cluster feature vectors\n",
    "    # Z = hierarchy.ward(features)\n",
    "    Z = hierarchy.linkage(\n",
    "        features, method='single', metric='cosine', optimal_ordering=True)\n",
    "    leaves = hierarchy.leaves_list(Z)\n",
    "\n",
    "    # methods:\n",
    "    # 'single', 'complete', 'average', 'weighted', 'centroid', 'median', 'ward'\n",
    "    # metrics:\n",
    "    # ‘braycurtis’, ‘canberra’, ‘chebyshev’, ‘cityblock’, ‘correlation’, ‘cosine’, \n",
    "    # ‘dice’, ‘euclidean’, ‘hamming’, ‘jaccard’, ‘jensenshannon’, ‘kulsinski’, \n",
    "    # ‘mahalanobis’, ‘matching’, ‘minkowski’, ‘rogerstanimoto’, ‘russellrao’, \n",
    "    # ‘seuclidean’, ‘sokalmichener’, ‘sokalsneath’, ‘sqeuclidean’, ‘yule’\n",
    "\n",
    "    return data[:, leaves]\n",
    "\n",
    "def get_lagged_data(data, n_lags=0, lag_type='concat', valid=True):\n",
    "    \"\"\"\n",
    "    data is T x N; for each time point t we concatenate\n",
    "    the observations at times t-1, ..., t-n_lags\n",
    "\n",
    "    Args:\n",
    "        data (T x num_features np array):\n",
    "        n_lags (int): 0 returns input data\n",
    "        lag_type (str): 'single' | 'concat'\n",
    "        valid (bool): only return valid data (ow 0-padding)\n",
    "\n",
    "    Returns:\n",
    "        np array: num_dataervations x (num_features * (n_lags + 1))\n",
    "    \"\"\"\n",
    "\n",
    "    data_lagged = [data]\n",
    "\n",
    "    for lag in range(n_lags):\n",
    "        data_lag_curr = np.roll(data, shift=lag + 1, axis=0)\n",
    "        data_lag_curr[0:lag + 1, :] = 0  # zero out first elements\n",
    "        data_lagged.append(data_lag_curr)\n",
    "\n",
    "    if lag_type == 'concat':\n",
    "        data = np.concatenate(data_lagged, axis=1)\n",
    "    elif lag_type == 'single':\n",
    "        data = data_lagged[n_lags]\n",
    "    else:\n",
    "        raise ValueError('\"%s\" invalid lag_type')\n",
    "    \n",
    "    if valid:\n",
    "        data = data[n_lags:]\n",
    "        \n",
    "    return data\n",
    "\n",
    "\n",
    "def pca_reduce_lag(data, n_components, n_lags, indxs):\n",
    "    # TODO!!!\n",
    "    \n",
    "    from sklearn.decomposition import PCA\n",
    "    \n",
    "    pca = PCA(n_components=n_components)\n",
    "    data_pca_ = pca.fit_transform(data)\n",
    "\n",
    "    data_pca = {}\n",
    "    for dtype in ['train', 'test', 'val']:\n",
    "        data_segs = []\n",
    "        for indx in indxs[dtype]:\n",
    "            data_segs.append(get_lagged_data(\n",
    "                data_pca_[(indx*trial_len):(indx*trial_len + trial_len)],\n",
    "                n_lags=n_lags))\n",
    "        data_pca[dtype] = data_segs\n",
    "\n",
    "    data_pca['train_all'] = np.concatenate(data_pca['train'], axis=0)\n",
    "    data_pca['val_all'] = np.concatenate(data_pca['val'], axis=0)\n",
    "    \n",
    "    return data_pca_, data_pca\n",
    "\n",
    "\n",
    "def plot_behavior_predictions(true, pred, slc=(0, 5000)):\n",
    "    \n",
    "    fig = plt.figure(figsize=(12, 4))\n",
    "    plt.plot(true[slice(*slc)], ls='-', color='k', label='True')\n",
    "    plt.plot(pred[slice(*slc)], ls='--', label='Predicted')\n",
    "    plt.xlabel('Time')\n",
    "    plt.yticks([])\n",
    "    plt.legend(frameon=False)\n",
    "    plt.show()\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data from matlab dict\n",
    "run_only = np.squeeze(mat_contents['trialFlag'] == 1)\n",
    "run_signal = np.squeeze(mat_contents['legs'].T[run_only])\n",
    "feed_signal = np.squeeze(mat_contents['feed'].T[run_only])\n",
    "if np.sum(feed_signal) > 0:\n",
    "    behavior = np.concatenate([run_signal[:, None], feed_signal[:, None]], axis=1)\n",
    "else:\n",
    "    behavior = run_signal\n",
    "data_run = zscore(mat_contents['dOO'].T[run_only])\n",
    "from scipy import signal\n",
    "data_run = signal.detrend(data_run, axis=0)\n",
    "data_run = cluster(data_run)\n",
    "\n",
    "# split into train/test trials\n",
    "trial_len = 100  # length of pseudo-trialsl\n",
    "n_trials = np.floor(data_run.shape[0] / trial_len)\n",
    "indxs = split_trials(\n",
    "    n_trials, trials_tr=10, trials_val=2, trials_test=0, trials_gap=0)\n",
    "data = {}\n",
    "run = {}\n",
    "for dtype in ['train', 'test', 'val']:\n",
    "    data_segs = []\n",
    "    run_segs = []\n",
    "    for indx in indxs[dtype]:\n",
    "        data_segs.append(data_run[(indx*trial_len):(indx*trial_len + trial_len)])\n",
    "        run_segs.append(run_signal[(indx*trial_len):(indx*trial_len + trial_len)])\n",
    "    data[dtype] = data_segs\n",
    "    run[dtype] = run_segs\n",
    "# for PCA/regression\n",
    "data['train_all'] = np.concatenate(data['train'], axis=0)\n",
    "data['val_all'] = np.concatenate(data['val'], axis=0)\n",
    "run['train_all'] = np.concatenate(run['train'], axis=0)\n",
    "run['val_all'] = np.concatenate(run['val'], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# first reduce dimensionality and lag\n",
    "n_components = 20\n",
    "n_lags = 0\n",
    "data_pca_lag_, data_pca_lag = pca_reduce_lag(data_run, n_components, n_lags, indxs)\n",
    "\n",
    "# chop off run data\n",
    "run_lag = {}\n",
    "for dtype in ['train', 'test', 'val']:\n",
    "    run_segs = []\n",
    "    for indx in indxs[dtype]:\n",
    "        run_segs.append(\n",
    "            run_signal[(indx*trial_len):(indx*trial_len + trial_len)][n_lags:])\n",
    "    run[dtype] = run_segs\n",
    "run_lag['train_all'] = np.concatenate(run['train'], axis=0)\n",
    "run_lag['val_all'] = np.concatenate(run['val'], axis=0)\n",
    "\n",
    "reg = Ridge(alpha=1e-5, normalize=True)\n",
    "reg.fit(data_pca_lag['train_all'], run_lag['train_all'])\n",
    "run_val = reg.predict(data_pca_lag['val_all'])\n",
    "\n",
    "r2 = r2_score(run_val, run_lag['val_all'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.12436323726823328\n"
     ]
    }
   ],
   "source": [
    "print(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4850, 80)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_pca_lag['train_all'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6726, 20)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_pca_lag_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'slice'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-036e74ddcb4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplot_behavior_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_lag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_pca_lag_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-b9548228196a>\u001b[0m in \u001b[0;36mplot_behavior_predictions\u001b[0;34m(true, pred, slc)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mslc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'k'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'True'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mslc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'--'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Predicted'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Time'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'slice'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 864x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plot_behavior_predictions(run_lag, reg.predict(data_pca_lag_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_behavior_predictions(\n",
    "    run_lag['val_all'], reg.predict(data_pca_lag['val_all']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_lags_list = [0, 1, 2, 3, 4, 6, 8, 10, 12]\n",
    "r2s = []\n",
    "for n_lags in n_lags_list:\n",
    "    # first reduce dimensionality and lag\n",
    "    n_components = 40\n",
    "    data_pca_lag_, data_pca_lag = pca_reduce_lag(\n",
    "        data_run, n_components, n_lags, indxs)\n",
    "    # chop off run data\n",
    "    run_lag = {}\n",
    "    for dtype in ['train', 'test', 'val']:\n",
    "        run_segs = []\n",
    "        for indx in indxs[dtype]:\n",
    "            run_segs.append(\n",
    "                run_signal[(indx*trial_len):(indx*trial_len + trial_len)][n_lags:])\n",
    "        run[dtype] = run_segs\n",
    "    run_lag['train_all'] = np.concatenate(run['train'], axis=0)\n",
    "    run_lag['val_all'] = np.concatenate(run['val'], axis=0)\n",
    "\n",
    "    reg = Ridge(alpha=1e-5, normalize=True)\n",
    "    reg.fit(data_pca_lag['train_all'], run_lag['train_all'])\n",
    "    run_val = reg.predict(data_pca_lag['val_all'])\n",
    "\n",
    "    r2s.append(r2_score(run_val, run_lag['val_all']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the r2 of the validation data\n",
    "fig = plt.figure(figsize=(6, 4))\n",
    "plt.plot(n_lags_list, r2s, ls='-', marker='o')\n",
    "plt.xlabel('Num lags')\n",
    "plt.ylabel('$R^2$')\n",
    "plt.title('Validation data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_behavior_predictions(\n",
    "    run_lag['val_all'], reg.predict(data_pca_lag['val_all']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs = np.reshape(reg.coef_, (13, 40))\n",
    "\n",
    "fig = plt.figure(figsize=(6, 4))\n",
    "plt.imshow(coeffs, cmap='RdBu')\n",
    "plt.ylabel('Num lags')\n",
    "plt.xlabel('PCs')\n",
    "plt.title('Regression weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project back into neuron space?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
